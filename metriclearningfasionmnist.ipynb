{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04a0a5f-11ab-40e1-b81b-bf7c3a15c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "import sys\n",
    "# sys.path.append(\"/Users/wren/Documents/personallearning/learningenv/lib/python3.12/site-packages\")\n",
    "sys.path.append(\"./myenv/lib/python3.12/site-packages\")\n",
    "# sys.path.append(\"./myenv/lib64/python3.11/site-packages\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from datetime import datetime  # Import datetime\n",
    "# !pip3 install seaborn --break-system-packages\n",
    "# !pip show seaborn\n",
    "# import seaborn as sns \n",
    "# --break-system-packages\n",
    "\n",
    "# !pip3 install scikit-learn --break-system-packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "# !pip3 install torchvision --break-system-packages\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, random_split, Subset, WeightedRandomSampler\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torchvision import transforms  # Import transforms from torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToPILImage  # Add import statement for ToPILImage\n",
    "\n",
    "# https://www.researchgate.net/figure/t-SNE-on-Fashion-MNIST-test-set_fig2_319312259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef5132c-a0b9-46f1-a3f3-a6faf3c01d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook --NotebookApp.extra_args=\"-Xfrozen_modules=off\"\n",
    "# %env PYDEVD_DISABLE_FILE_VALIDATION=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8ebfe2-7d9f-4850-a926-870425b34e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matched Network Model\n",
    "# regularizaation and normalization....architecture changes\n",
    "class MatchedNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MatchedNetwork, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Define the fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 3 * 3, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128))\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        # Forward pass through the convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "        # Reshape the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Forward pass through the fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        # Forward pass through each branch of the network\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        output3 = self.forward_once(input3)\n",
    "        return output1, output2, output3\n",
    "        \n",
    "class TripletMNIST(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_img, anchor_label = self.dataset[index]\n",
    "        \n",
    "        # Choose positive sample with the same label as anchor\n",
    "        positive_index = index\n",
    "        while positive_index == index:\n",
    "            positive_index = random.randint(0, len(self.dataset) - 1)\n",
    "        positive_img, positive_label = self.dataset[positive_index]\n",
    "\n",
    "        # Ensure the positive sample has the same label as the anchor\n",
    "        while positive_label != anchor_label:\n",
    "            positive_index = random.randint(0, len(self.dataset) - 1)\n",
    "            positive_img, positive_label = self.dataset[positive_index]\n",
    "\n",
    "        # Choose negative sample with different label from anchor\n",
    "        negative_index = index\n",
    "        while negative_index == index or self.dataset[negative_index][1] == anchor_label:\n",
    "            negative_index = random.randint(0, len(self.dataset) - 1)\n",
    "        negative_img, negative_label = self.dataset[negative_index]\n",
    "\n",
    "        return anchor_img, positive_img, negative_img, anchor_label, positive_label, negative_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123673f5-e5e9-40bb-b8df-514cb386d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Matched network, criterion, and optimizer\n",
    "learning_rate = 0.001\n",
    "matched_model = MatchedNetwork()\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "optimizer = optim.Adam(matched_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "full_dataset = FashionMNIST(root='./data', train=True, transform=ToTensor())\n",
    "\n",
    "\n",
    "# sampler isn't working yet\n",
    "\n",
    "# Count the occurrences of each class in the dataset\n",
    "class_counts = torch.zeros(len(full_dataset.classes))\n",
    "for _, label in full_dataset:\n",
    "    class_counts[label] += 1\n",
    "\n",
    "# Compute total number of samples\n",
    "total_samples = sum(class_counts)\n",
    "\n",
    "# Compute class weights for balancing\n",
    "class_weights = total_samples / class_counts\n",
    "\n",
    "# Print out the class weights\n",
    "print(\"Class Weights:\")\n",
    "for i, weight in enumerate(class_weights):\n",
    "    print(f\"Class {i}: {weight.item()}\")\n",
    "\n",
    "# Define sampler for class balancing\n",
    "fashionsampler = WeightedRandomSampler(class_frequencies, len(full_dataset))\n",
    "\n",
    "\n",
    "\n",
    "# Choose the size of the subset\n",
    "subset_size = 1000\n",
    "\n",
    "# Randomly select indices for the subset\n",
    "subset_indices = torch.randperm(len(full_dataset))[:subset_size]\n",
    "\n",
    "# Create a subset of the dataset\n",
    "subset_dataset = torch.utils.data.Subset(full_dataset, subset_indices)\n",
    "\n",
    "# Create triplet dataset\n",
    "triplet_dataset = TripletMNIST(subset_dataset)\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.8 * len(triplet_dataset))\n",
    "val_size = len(triplet_dataset) - train_size\n",
    "train_subset, val_subset = random_split(triplet_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for the train and validation sets with triplets\n",
    "train_loader_triplet = DataLoader(train_subset, batch_size=64, shuffle=True)# sampler=fashionsampler\n",
    "val_loader_triplet = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aacfdc5-8e89-4c14-bdab-67ef3ba1d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(loader, dataset, title):\n",
    "    # Initialize empty list to store class labels\n",
    "    class_labels = []\n",
    "\n",
    "    # Iterate over the loader to extract class labels\n",
    "    for _, _, _, anchor_label, _, _ in loader:\n",
    "        class_labels.extend(anchor_label.numpy().tolist())\n",
    "\n",
    "    # Plot histogram of class distribution\n",
    "    plt.hist(class_labels, bins=len(dataset.classes), color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Class Label')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(range(len(dataset.classes)), dataset.classes, rotation=45, ha='right')\n",
    "\n",
    "# Create subplots for side-by-side plotting\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot training dataset distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_class_distribution(train_loader_triplet, full_dataset, 'Training Dataset Class Distribution')\n",
    "\n",
    "# Plot validation dataset distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_class_distribution(val_loader_triplet, full_dataset, 'Validation Dataset Class Distribution')\n",
    "\n",
    "# Adjust layout and display plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f900ed-1635-4259-b9bb-47a3265652cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(loader, dataset, title):\n",
    "    # Initialize empty list to store class labels\n",
    "    class_labels = []\n",
    "\n",
    "    # Iterate over the loader to extract class labels\n",
    "    for _, _, _, anchor_label, _, _ in loader:\n",
    "        class_labels.extend(anchor_label.numpy().tolist())\n",
    "\n",
    "    # Plot histogram of class distribution\n",
    "    plt.hist(class_labels, bins=len(dataset.classes), color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Class Label')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(range(len(dataset.classes)), dataset.classes, rotation=45, ha='right')\n",
    "\n",
    "# Create subplots for side-by-side plotting\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot original training dataset distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_class_distribution(DataLoader(train_subset, batch_size=64), full_dataset, 'Original Training Dataset Class Distribution')\n",
    "\n",
    "# Plot original validation dataset distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_class_distribution(DataLoader(val_subset, batch_size=64), full_dataset, 'Original Validation Dataset Class Distribution')\n",
    "\n",
    "# Plot balanced training dataset distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_class_distribution(train_loader_triplet, full_dataset, 'Balanced Training Dataset Class Distribution')\n",
    "\n",
    "# Plot balanced validation dataset distribution\n",
    "plt.subplot(2, 2, 4)\n",
    "plot_class_distribution(val_loader_triplet, full_dataset, 'Balanced Validation Dataset Class Distribution')\n",
    "\n",
    "# Adjust layout and display plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4781d3-a7a2-4661-884c-271120e39707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label names for FashionMNIST\n",
    "label_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Create an instance of TripletMNIST\n",
    "triplet_dataset = TripletMNIST(full_dataset)\n",
    "\n",
    "# Choose an index to get an example\n",
    "index = 3\n",
    "\n",
    "# Get the anchor, positive, and negative samples along with their labels\n",
    "anchor_img, positive_img, negative_img, anchor_label, positive_label, negative_label = triplet_dataset[index]\n",
    "\n",
    "# Map label indices to label names\n",
    "anchor_label_name = label_names[anchor_label]\n",
    "positive_label_name = label_names[positive_label]\n",
    "negative_label_name = label_names[negative_label]\n",
    "\n",
    "# Display the anchor, positive, and negative samples\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(anchor_img.squeeze(), cmap='gray')\n",
    "axes[0].set_title('Anchor')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(positive_img.squeeze(), cmap='gray')\n",
    "axes[1].set_title('Positive')\n",
    "axes[1].axis('off')\n",
    "axes[2].imshow(negative_img.squeeze(), cmap='gray')\n",
    "axes[2].set_title('Negative')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Add suptitle with the actual label names\n",
    "fig.suptitle(f'Labels: Anchor - {anchor_label_name}, Positive - {positive_label_name}, Negative - {negative_label_name}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9fcad3-e0de-4383-81c1-43a3438a8ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label names for FashionMNIST\n",
    "label_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Inspect the structure of a batch\n",
    "for batch in train_loader_triplet:\n",
    "    # Extract images and labels from the batch\n",
    "    anchor_images = batch[0]\n",
    "    positive_images = batch[1]\n",
    "    negative_images = batch[2]\n",
    "\n",
    "    # Print the shape of the images tensor\n",
    "    print(\"Anchor images shape:\", anchor_images.shape)\n",
    "    print(\"Positive images shape:\", positive_images.shape)\n",
    "    print(\"Negative images shape:\", negative_images.shape)\n",
    "\n",
    "    # Plot several images in subplots\n",
    "    num_images = 5  # Number of images to plot\n",
    "    fig, axes = plt.subplots(3, num_images, figsize=(12, 6))\n",
    "\n",
    "    # Plot each image with its corresponding label name as the title\n",
    "    for i in range(num_images):\n",
    "        # Anchor image\n",
    "        ax = axes[0, i]\n",
    "        ax.imshow(anchor_images[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "        ax.axis('off')  # Hide axis\n",
    "        ax.set_title(\"Anchor\")\n",
    "\n",
    "        # Positive image\n",
    "        ax = axes[1, i]\n",
    "        ax.imshow(positive_images[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "        ax.axis('off')  # Hide axis\n",
    "        ax.set_title(\"Positive\")\n",
    "\n",
    "        # Negative image\n",
    "        ax = axes[2, i]\n",
    "        ax.imshow(negative_images[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "        ax.axis('off')  # Hide axis\n",
    "        ax.set_title(\"Negative\")\n",
    "\n",
    "    plt.show()\n",
    "    break  # Only print the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c91350-bb2a-4f18-81e6-c592dbaa58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the structure of a batch\n",
    "for batch in train_loader_triplet:\n",
    "    print(\"Batch structure:\", len(batch))\n",
    "    break  # Only inspect the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c9cae-52ee-4632-888f-6bbc1c3801ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the learning rate to see what the errors are\n",
    "# try different loss functions with distance metrics\n",
    "\n",
    "# Set the hyperparameters \n",
    "num_epochs = 50\n",
    "best_val_loss = np.inf\n",
    "patience = 10\n",
    "\n",
    "# Initialize the loss and accuracy captures\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    matched_model.train()\n",
    "    total_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for batch in train_loader_triplet:\n",
    "        optimizer.zero_grad()\n",
    "        anchor_imgs, positive_imgs, negative_imgs = batch[:3]\n",
    "        output1, output2, output3 = matched_model(anchor_imgs, positive_imgs, negative_imgs)\n",
    "        loss = criterion(output1, output2, output3)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        _, predicted = torch.max(output1, 1)  # Assuming output1 is the output of the model\n",
    "        total_train += anchor_imgs.size(0)\n",
    "        correct_train += (predicted == batch[3]).sum().item()  # Assuming true labels are at index 3 in the batch\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader_triplet)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation loop\n",
    "    matched_model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader_triplet:\n",
    "            anchor_imgs, positive_imgs, negative_imgs = batch[:3]\n",
    "            output1, output2, output3 = matched_model(anchor_imgs, positive_imgs, negative_imgs)\n",
    "            loss = criterion(output1, output2, output3)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            # Compute validation accuracy\n",
    "            _, predicted = torch.max(output1, 1)  # Assuming output1 is the output of the model\n",
    "            total_val += anchor_imgs.size(0)\n",
    "            correct_val += (predicted == batch[3]).sum().item()  # Assuming true labels are at index 3 in the batch\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader_triplet)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracy = correct_val / total_val\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {round(avg_train_loss,4)}, Validation Loss: {round(avg_val_loss,4)}, Training Accuracy: {round(train_accuracy,4)}, Validation Accuracy: {round(val_accuracy,4)}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Validation loss did not improve for {patience} epochs. Early stopping...\")\n",
    "            break\n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efc545-12b5-4e9a-a9e7-5f3172b24f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c172c-09d8-4850-a8c8-47c96a04f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label names for FashionMNIST\n",
    "label_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Initialize lists to store true labels and predicted scores\n",
    "true_labels = []\n",
    "predicted_scores = []\n",
    "\n",
    "# Iterate through the data loader to get predictions\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader_triplet:\n",
    "        anchor_imgs, positive_imgs, negative_imgs, anchor_labels, _, _ = batch\n",
    "        output = matched_model.forward_once(anchor_imgs)\n",
    "        true_labels.extend(anchor_labels.numpy())\n",
    "        predicted_scores.extend(output.numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_scores = np.array(predicted_scores)\n",
    "\n",
    "# Calculate the number of classes\n",
    "num_classes = len(np.unique(true_labels))\n",
    "\n",
    "# Plot ROC curves for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for class_label in range(num_classes):\n",
    "    # Extract true labels and predicted scores for the current class\n",
    "    class_true_labels = (true_labels == class_label).astype(int)\n",
    "    class_predicted_scores = predicted_scores[:, class_label]\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(class_true_labels, class_predicted_scores)\n",
    "    \n",
    "    # Calculate AUROC score\n",
    "    auroc = roc_auc_score(class_true_labels, class_predicted_scores)\n",
    "    \n",
    "    # Plot ROC curve with label names\n",
    "    plt.plot(fpr, tpr, label=f'{label_names[class_label]} (AUROC = {auroc:.2f})')\n",
    "\n",
    "# Plot random guess line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for FashionMNIST Classes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f734d537-6cb4-4e03-b2ea-ea1e4c9e6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "matched_model.eval()\n",
    "\n",
    "# Initialize empty lists to store true and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Iterate over the validation dataset\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader_triplet:\n",
    "        # Unpack the batch\n",
    "        anchor_imgs, positive_imgs, negative_imgs = batch[:3]  # Adjust unpacking based on batch structure\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        outputs = matched_model(anchor_imgs, positive_imgs, negative_imgs) \n",
    "        \n",
    "        # Extract the predicted labels\n",
    "        _, predicted = torch.max(outputs[0], 1)  # Assuming outputs is a tuple of output tensors\n",
    "        print(predicted)\n",
    "        \n",
    "        # Append true labels and predicted labels\n",
    "        true_labels.append(batch[3].numpy())  # Assuming true labels are at index 3 in the batch\n",
    "        predicted_labels.append(predicted.cpu().numpy())\n",
    "\n",
    "# # Concatenate true labels and predicted labels across all batches\n",
    "# true_labels = np.concatenate(true_labels)\n",
    "# predicted_labels = np.concatenate(predicted_labels)\n",
    "\n",
    "\n",
    "# # Define label names for FashionMNIST\n",
    "# label_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "#                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# # Function to compute and visualize confusion matrix\n",
    "# def plot_confusion_matrix(true_labels, predicted_labels, label_names):\n",
    "#     # Compute confusion matrix\n",
    "#     cm = confusion_matrix(true_labels, predicted_labels)\n",
    "#     # Plot confusion matrix as heatmap\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=label_names, yticklabels=label_names)\n",
    "#     plt.xlabel('Predicted Labels')\n",
    "#     plt.ylabel('True Labels')\n",
    "#     plt.title('Confusion Matrix')\n",
    "#     plt.show()\n",
    "\n",
    "# # Plot confusion matrix\n",
    "# plot_confusion_matrix(true_labels, predicted_labels, label_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbbdee-340e-4a68-9aad-40906a26accd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e2fec-34cc-4307-906d-96c00189d8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eaebd5-dd7c-4813-8af1-2152793955bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a94cd7d-bdef-403e-a5fc-ff1c4ea88fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339566f2-1f37-4903-ae6b-6ee759cf559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212732c-5742-4ec1-b824-c6a76b66d964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cad66d-08d4-4e94-927a-6bbeb29a17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label names for FashionMNIST\n",
    "label_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "# Define a function to extract and visualize embeddings\n",
    "def visualize_embeddings(loader, title):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            anchor_imgs, positive_imgs, negative_imgs = batch[:3]\n",
    "            output1, output2, output3 = matched_model(anchor_imgs, positive_imgs, negative_imgs)\n",
    "            embeddings.extend([output1.numpy(), output2.numpy(), output3.numpy()])\n",
    "            labels.extend([batch[3].numpy(), batch[4].numpy(), batch[5].numpy()])  # Assuming the anchor, positive, and negative labels are at index 3, 4, and 5\n",
    "\n",
    "    # Concatenate embeddings and labels\n",
    "    embeddings = np.concatenate(embeddings)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Visualize projected embeddings\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for label_idx, label_name in enumerate(label_names):\n",
    "        plt.scatter(embeddings_2d[labels == label_idx, 0], embeddings_2d[labels == label_idx, 1], label=label_name)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize embeddings for training set\n",
    "visualize_embeddings(train_loader_triplet, 'Projected Embeddings for Training Set')\n",
    "\n",
    "# Visualize embeddings for validation set\n",
    "visualize_embeddings(val_loader_triplet, 'Projected Embeddings for Validation Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a28e81-f055-4a78-8e6c-f43a97b1acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the list of clusters to visualize\n",
    "clsuters_to_visualize = [3,4]\n",
    "\n",
    "# Define a function to extract and visualize embeddings\n",
    "def visualize_embeddings(loader, title, labels_to_project):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            anchor_imgs, positive_imgs, negative_imgs = batch[:3]\n",
    "            output1, output2, output3 = matched_model(anchor_imgs, positive_imgs, negative_imgs)\n",
    "            embeddings.extend([output1.numpy(), output2.numpy(), output3.numpy()])\n",
    "            labels.extend([batch[3].numpy(), batch[4].numpy(), batch[5].numpy()])  # Assuming the anchor, positive, and negative labels are at index 3, 4, and 5\n",
    "\n",
    "    # Concatenate embeddings and labels\n",
    "    embeddings = np.concatenate(embeddings)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    # Filter embeddings and labels for the specified labels to project\n",
    "    filter_mask = np.isin(labels, labels_to_project)\n",
    "    filtered_embeddings = embeddings[filter_mask]\n",
    "    filtered_labels = labels[filter_mask]\n",
    "\n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(filtered_embeddings)\n",
    "    \n",
    "    # Visualize projected embeddings\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for label_idx, label_name in enumerate(label_names):\n",
    "        if label_idx in labels_to_project:\n",
    "            plt.scatter(embeddings_2d[filtered_labels == label_idx, 0], embeddings_2d[filtered_labels == label_idx, 1], label=label_name)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize embeddings for labels 3 and 4 in the training set\n",
    "visualize_embeddings(train_loader_triplet, 'Projected Embeddings for Training Set (Labels {0}'.format(clsuters_to_visualize), clsuters_to_visualize)\n",
    "\n",
    "# Visualize embeddings for labels 3 and 4 in the validation set\n",
    "visualize_embeddings(val_loader_triplet, 'Projected Embeddings for Validation Set (Labels {0}'.format(clsuters_to_visualize), clsuters_to_visualize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f369d412-375f-4ff4-b598-54e1745cb29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the clusters to visualize\n",
    "clusters_to_visualize = [3,4]\n",
    "\n",
    "# Define a function to extract and visualize embeddings\n",
    "def visualize_embeddings_with_images(loader, model, title, labels_to_display):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title(title)\n",
    "\n",
    "    # Keep track of the number of embeddings displayed\n",
    "    num_embeddings_displayed = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            anchor_imgs, _, _ = batch[:3]  # Only need anchor images\n",
    "            outputs = model.forward_once(anchor_imgs)\n",
    "            for embedding, label, image in zip(outputs, batch[3], anchor_imgs):\n",
    "                # Check if the label matches the ones we want to display\n",
    "                if label.item() in labels_to_display:\n",
    "                    # Plot the embedding\n",
    "                    plt.scatter(embedding[0], embedding[1], color='blue', label=f'Label: {label.item()}')\n",
    "\n",
    "                    # Convert PyTorch tensor to NumPy array\n",
    "                    image_np = image.squeeze().cpu().numpy()\n",
    "                    # Resize image using OpenCV\n",
    "                    resized_image = cv2.resize(image_np, (32, 32))  # Resize to (32, 32)\n",
    "\n",
    "                    # Create an AnnotationBbox with the image\n",
    "                    imagebox = OffsetImage(resized_image, zoom=1)\n",
    "                    ab = AnnotationBbox(imagebox, (embedding[0], embedding[1]), frameon=False)\n",
    "                    plt.gca().add_artist(ab)\n",
    "\n",
    "                    num_embeddings_displayed += 1\n",
    "\n",
    "            # Break if displayed enough embeddings\n",
    "            if num_embeddings_displayed == len(labels_to_display):\n",
    "                break\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    # plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming matched_model is the trained Matched network model\n",
    "# Assuming train_loader_triplet is the DataLoader containing the training data\n",
    "visualize_embeddings_with_images(train_loader_triplet, matched_model, \n",
    "                                 'Projected Embeddings with Images for Training Set (Clusters {0})'.format(clusters_to_visualize),\n",
    "                                  clusters_to_visualize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a2801-8c0d-4b36-a294-732ad99fe8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to do the gradcam\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.model.eval()\n",
    "        self.feature_grad = None\n",
    "        self.hook = self.register_hooks()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.feature_grad = grad_out[0]\n",
    "\n",
    "        hook = self.target_layer.register_forward_hook(forward_hook)\n",
    "        _ = self.target_layer.register_backward_hook(backward_hook)\n",
    "        return hook\n",
    "\n",
    "    def generate_grad_cam(self, input_image1, input_image2, input_image3, target_class=None):\n",
    "        self.model.zero_grad()\n",
    "        output1, _, _ = self.model(input_image1, input_image2, input_image3)  # Forward pass through the Matched network\n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(output1, dim=1)\n",
    "\n",
    "        # Compute gradients for the target class\n",
    "        one_hot_output = torch.zeros_like(output1)\n",
    "        one_hot_output[torch.arange(len(output1)), target_class] = 1\n",
    "        output1.backward(gradient=one_hot_output, retain_graph=True)\n",
    "\n",
    "        # Compute Grad-CAM\n",
    "        weights = torch.mean(self.feature_grad, dim=(2, 3), keepdim=True)\n",
    "        grad_cam = torch.mean(weights * self.feature_grad, dim=1).squeeze()\n",
    "        grad_cam = F.relu(grad_cam)\n",
    "\n",
    "        # Normalize Grad-CAM\n",
    "        grad_cam = (grad_cam - torch.min(grad_cam)) / (torch.max(grad_cam) - torch.min(grad_cam) + 1e-8)\n",
    "\n",
    "        return grad_cam.detach().cpu().numpy()\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "# Define a function to overlay the grad cam on the original image\n",
    "def overlay_grad_cam(input_image, grad_cam_map, alpha=0.5):\n",
    "    # Convert the input image to a numpy array and scale it to the range [0, 255]\n",
    "    input_image_np = (input_image.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "    \n",
    "    # Resize the Grad-CAM map to match the input image dimensions\n",
    "    grad_cam_map_resized = cv2.resize(grad_cam_map, (input_image_np.shape[1], input_image_np.shape[0]))\n",
    "    \n",
    "    # Apply the colormap to the resized Grad-CAM map\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * grad_cam_map_resized), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    \n",
    "    # Convert the input image to a three-channel image\n",
    "    input_image_resized = cv2.cvtColor(input_image_np, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Blend the resized input image and the heatmap using alpha blending\n",
    "    output_image = cv2.addWeighted(input_image_resized, alpha, heatmap, 1 - alpha, 0, dtype=cv2.CV_8U)\n",
    "    \n",
    "    return input_image_resized, output_image\n",
    "\n",
    "# Assuming model is your MatchedNetwork model\n",
    "grad_cam = GradCAM(matched_model, matched_model.conv_layers[-3])  # Assuming last convolutional layer is target layer\n",
    "\n",
    "# Assuming dataset is your TripletMNIST dataset\n",
    "for anchor_img, positive_img, negative_img, anchor_label, _, _ in triplet_dataset:\n",
    "\n",
    "    # Pass the anchor, positive, and negative images to the GradCAM object to generate Grad-CAM\n",
    "    grad_cam_map = grad_cam.generate_grad_cam(anchor_img.unsqueeze(0), positive_img.unsqueeze(0), negative_img.unsqueeze(0), target_class=anchor_label)\n",
    "    \n",
    "    # Overlay Grad-CAM on input image and display\n",
    "    input_image, output_image = overlay_grad_cam(anchor_img, grad_cam_map)\n",
    "    \n",
    "    # Plot original image and overlay image side by side\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(input_image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(output_image)\n",
    "    plt.title('Grad-CAM Overlay')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    break  # Break after visualizing the first image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2c743-aaa5-4575-96f0-29de21a6c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018dd8b-5177-408a-80b6-0b384fb6c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to overlay the grad cam on the original image\n",
    "def overlay_grad_cam(input_image, grad_cam_map, alpha=0.5):\n",
    "    # Convert the input image to a numpy array and scale it to the range [0, 255]\n",
    "    input_image_np = (input_image.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "    \n",
    "    # Resize the Grad-CAM map to match the input image dimensions\n",
    "    grad_cam_map_resized = cv2.resize(grad_cam_map, (input_image_np.shape[1], input_image_np.shape[0]))\n",
    "    \n",
    "    # Convert the input image to a three-channel image\n",
    "    input_image_resized = cv2.cvtColor(input_image_np, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Apply colormap to the Grad-CAM map\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * grad_cam_map_resized), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    \n",
    "    # Blend the input image and the heatmap using alpha blending\n",
    "    output_image = cv2.addWeighted(input_image_resized, 1-alpha, heatmap, alpha, 0, dtype=cv2.CV_8U)\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "# Assuming model is your MatchedNetwork model\n",
    "grad_cam = GradCAM(matched_model, matched_model.conv_layers[-1])  # Start from the last convolutional layer\n",
    "\n",
    "# Iterate over each convolutional layer in the model\n",
    "for layer_idx, layer in enumerate(matched_model.conv_layers):\n",
    "    # Update the target layer for Grad-CAM\n",
    "    grad_cam.target_layer = layer\n",
    "    \n",
    "    # Assuming dataset is your TripletMNIST dataset\n",
    "    for anchor_img, positive_img, negative_img, anchor_label, _, _ in triplet_dataset:\n",
    "        # Pass the anchor, positive, and negative images to the GradCAM object to generate Grad-CAM\n",
    "        grad_cam_map = grad_cam.generate_grad_cam(anchor_img.unsqueeze(0), positive_img.unsqueeze(0), negative_img.unsqueeze(0), target_class=anchor_label)\n",
    "        \n",
    "        # Overlay Grad-CAM on input image and display\n",
    "        output_image = overlay_grad_cam(anchor_img, grad_cam_map)\n",
    "        \n",
    "        # Plot original image and overlay image side by side\n",
    "        plt.figure(figsize=(6,3))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(anchor_img.squeeze(), cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(output_image)\n",
    "        plt.title(f'Grad-CAM Overlay - Layer {layer_idx}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        break  # Break after visualizing the Grad-CAM for the first image\n",
    "        \n",
    "    # break  # Break after visualizing Grad-CAM for the first layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828ca5f-bb1b-40bd-b3ed-bfa9ec2215c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract feature maps from a given layer\n",
    "def get_feature_maps(model, input_image):\n",
    "    feature_maps = []\n",
    "    x = input_image.unsqueeze(0)\n",
    "    for layer in model.conv_layers:\n",
    "        x = layer(x)\n",
    "        feature_maps.append(x)\n",
    "    return feature_maps\n",
    "\n",
    "# Assuming model is your MatchedNetwork model\n",
    "# Assuming dataset is your TripletMNIST dataset\n",
    "for anchor_img, positive_img, negative_img, anchor_label, _, _ in triplet_dataset:\n",
    "    # Pass the anchor image through the model\n",
    "    feature_maps = get_feature_maps(matched_model, anchor_img)\n",
    "\n",
    "    # Visualize feature maps for each layer\n",
    "    for i, fmap in enumerate(feature_maps):\n",
    "        num_features = fmap.size(1)\n",
    "        num_rows = 1\n",
    "        num_cols = num_features\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 1))\n",
    "        for j in range(num_features):\n",
    "            axs[j].imshow(fmap[0, j].detach().cpu(), cmap='gray')\n",
    "            axs[j].axis('off')\n",
    "            if j == 0:\n",
    "                axs[j].set_title(f'Layer {i+1}')\n",
    "        plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f447a-c877-45c9-8b8b-c336a4f56897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain embeddings for each image in the dataset\n",
    "class_embeddings = {i: [] for i in range(10)}  # Dictionary to store embeddings grouped by class\n",
    "with torch.no_grad():\n",
    "    for anchor_img, positive_img, negative_img, anchor_label, positive_label, negative_label in triplet_dataset:\n",
    "        anchor_emb = matched_model.forward_once(anchor_img.unsqueeze(0))\n",
    "        class_embeddings[anchor_label].append(anchor_emb)\n",
    "\n",
    "def compute_pairwise_similarities(embeddings):\n",
    "    similarities = {}\n",
    "    for class_label, emb_list in embeddings.items():\n",
    "        emb_array = torch.stack(emb_list)  # Stack tensors along a new dimension\n",
    "        num_samples = emb_array.shape[0]\n",
    "        emb_array = emb_array.view(num_samples, -1)  # Flatten the tensor\n",
    "        sim_matrix = cosine_similarity(emb_array.detach().numpy())  # Compute cosine similarity\n",
    "        np.fill_diagonal(sim_matrix, 0)\n",
    "        pairwise_similarities = sim_matrix.sum(axis=1) / (num_samples - 1)\n",
    "        similarities[class_label] = pairwise_similarities.mean()\n",
    "    return similarities\n",
    "\n",
    "# Compute pairwise similarities within each class\n",
    "similarities = compute_pairwise_similarities(class_embeddings)\n",
    "\n",
    "# Plot the metric for embedding similarities within classes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(similarities.keys(), similarities.values())\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Mean Cosine Similarity')\n",
    "plt.title('Mean Cosine Similarity of Embeddings within Each Class (FashionMNIST)')\n",
    "plt.xticks(list(range(10)), ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0979e-5783-446f-a9a4-ac12e2f98594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming class_embeddings is a dictionary containing embeddings grouped by class\n",
    "class_label = 3  # Class label for which you want to examine embeddings\n",
    "\n",
    "# Get embeddings for class 3\n",
    "embeddings_class_3 = class_embeddings[class_label]\n",
    "\n",
    "# Plot a few embeddings from class 3\n",
    "num_embeddings_to_plot = min(5, len(embeddings_class_3))  # Plot at most 5 embeddings\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(num_embeddings_to_plot):\n",
    "    plt.subplot(1, num_embeddings_to_plot, i + 1)\n",
    "    plt.imshow(embeddings_class_3[i], cmap='gray')\n",
    "    plt.title(f'Embedding {i+1}')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Embeddings for Class 3', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7bea3e-7d24-4a35-a8b2-f06ec08c5a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb44b4-0dcb-48e1-9d94-e67d112c44a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20548323-03a0-4f4b-a34b-eb02364abe28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73203169-cbde-46e3-8303-41a7902bcd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
